---
# Source: lgtm-minimal/charts/minio/templates/networkpolicy.yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: lgtm-minio
  namespace: "default"
  labels:
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: minio
    app.kubernetes.io/version: 2024.9.22
    helm.sh/chart: minio-14.7.11
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/instance: lgtm
      app.kubernetes.io/name: minio
  policyTypes:
    - Ingress
    - Egress
  egress:
    - {}
  ingress:
    # Allow inbound connections
    - ports:
        - port: 9001
        - port: 9000
---
# Source: lgtm-minimal/charts/minio/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: lgtm-minio
  namespace: "default"
  labels:
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: minio
    app.kubernetes.io/version: 2024.9.22
    helm.sh/chart: minio-14.7.11
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: lgtm
      app.kubernetes.io/name: minio
---
# Source: lgtm-minimal/charts/grafana/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
automountServiceAccountToken: false
metadata:
  labels:
    helm.sh/chart: grafana-8.5.1
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/version: "11.2.0"
    app.kubernetes.io/managed-by: Helm
  name: lgtm-grafana
  namespace: default
---
# Source: lgtm-minimal/charts/loki-stack/charts/loki/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app: loki
    chart: loki-2.16.0
    release: lgtm
    heritage: Helm
  annotations:
    {}
  name: lgtm-loki
  namespace: default
automountServiceAccountToken: true
---
# Source: lgtm-minimal/charts/minio/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: lgtm-minio
  namespace: "default"
  labels:
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: minio
    app.kubernetes.io/version: 2024.9.22
    helm.sh/chart: minio-14.7.11
automountServiceAccountToken: false
secrets:
  - name: lgtm-minio
---
# Source: lgtm-minimal/charts/tempo/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: lgtm-tempo
  namespace: default
  labels:
    helm.sh/chart: tempo-1.10.3
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/version: "2.5.0"
    app.kubernetes.io/managed-by: Helm
automountServiceAccountToken: true
---
# Source: lgtm-minimal/charts/grafana/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: lgtm-grafana
  namespace: default
  labels:
    helm.sh/chart: grafana-8.5.1
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/version: "11.2.0"
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  
  admin-user: "YWRtaW4="
  admin-password: "YWRtaW4="
  ldap-toml: ""
---
# Source: lgtm-minimal/charts/loki-stack/charts/loki/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: lgtm-loki
  namespace: default
  labels:
    app: loki
    chart: loki-2.16.0
    release: lgtm
    heritage: Helm
data:
  loki.yaml: YXV0aF9lbmFibGVkOiBmYWxzZQpjaHVua19zdG9yZV9jb25maWc6CiAgbWF4X2xvb2tfYmFja19wZXJpb2Q6IDBzCmNvbXBhY3RvcjoKICBzaGFyZWRfc3RvcmU6IGZpbGVzeXN0ZW0KICB3b3JraW5nX2RpcmVjdG9yeTogL2RhdGEvbG9raS9ib2x0ZGItc2hpcHBlci1jb21wYWN0b3IKaW5nZXN0ZXI6CiAgY2h1bmtfYmxvY2tfc2l6ZTogMjYyMTQ0CiAgY2h1bmtfaWRsZV9wZXJpb2Q6IDNtCiAgY2h1bmtfcmV0YWluX3BlcmlvZDogMW0KICBsaWZlY3ljbGVyOgogICAgcmluZzoKICAgICAgcmVwbGljYXRpb25fZmFjdG9yOiAxCiAgbWF4X3RyYW5zZmVyX3JldHJpZXM6IDAKICB3YWw6CiAgICBkaXI6IC9kYXRhL2xva2kvd2FsCmxpbWl0c19jb25maWc6CiAgZW5mb3JjZV9tZXRyaWNfbmFtZTogZmFsc2UKICBtYXhfZW50cmllc19saW1pdF9wZXJfcXVlcnk6IDUwMDAKICByZWplY3Rfb2xkX3NhbXBsZXM6IHRydWUKICByZWplY3Rfb2xkX3NhbXBsZXNfbWF4X2FnZTogMTY4aAptZW1iZXJsaXN0OgogIGpvaW5fbWVtYmVyczoKICAtICdsZ3RtLWxva2ktbWVtYmVybGlzdCcKc2NoZW1hX2NvbmZpZzoKICBjb25maWdzOgogIC0gZnJvbTogIjIwMjAtMTAtMjQiCiAgICBpbmRleDoKICAgICAgcGVyaW9kOiAyNGgKICAgICAgcHJlZml4OiBpbmRleF8KICAgIG9iamVjdF9zdG9yZTogZmlsZXN5c3RlbQogICAgc2NoZW1hOiB2MTEKICAgIHN0b3JlOiBib2x0ZGItc2hpcHBlcgpzZXJ2ZXI6CiAgZ3JwY19saXN0ZW5fcG9ydDogOTA5NQogIGh0dHBfbGlzdGVuX3BvcnQ6IDMxMDAKc3RvcmFnZV9jb25maWc6CiAgYm9sdGRiX3NoaXBwZXI6CiAgICBhY3RpdmVfaW5kZXhfZGlyZWN0b3J5OiAvZGF0YS9sb2tpL2JvbHRkYi1zaGlwcGVyLWFjdGl2ZQogICAgY2FjaGVfbG9jYXRpb246IC9kYXRhL2xva2kvYm9sdGRiLXNoaXBwZXItY2FjaGUKICAgIGNhY2hlX3R0bDogMjRoCiAgICBzaGFyZWRfc3RvcmU6IGZpbGVzeXN0ZW0KICBmaWxlc3lzdGVtOgogICAgZGlyZWN0b3J5OiAvZGF0YS9sb2tpL2NodW5rcwp0YWJsZV9tYW5hZ2VyOgogIHJldGVudGlvbl9kZWxldGVzX2VuYWJsZWQ6IGZhbHNlCiAgcmV0ZW50aW9uX3BlcmlvZDogMHM=
---
# Source: lgtm-minimal/charts/minio/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: lgtm-minio
  namespace: "default"
  labels:
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: minio
    app.kubernetes.io/version: 2024.9.22
    helm.sh/chart: minio-14.7.11
type: Opaque
data:
  root-user: "YWRtaW4="
  root-password: "c3VwZXJzZWNyZXQ="
---
# Source: lgtm-minimal/charts/grafana/templates/configmap-dashboard-provider.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    helm.sh/chart: grafana-8.5.1
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/version: "11.2.0"
    app.kubernetes.io/managed-by: Helm
  name: lgtm-grafana-config-dashboards
  namespace: default
data:
  provider.yaml: |-
    apiVersion: 1
    providers:
      - name: 'sidecarProvider'
        orgId: 1
        folder: ''
        folderUid: ''
        type: file
        disableDeletion: false
        allowUiUpdates: false
        updateIntervalSeconds: 30
        options:
          foldersFromFilesStructure: false
          path: /tmp/dashboards
---
# Source: lgtm-minimal/charts/grafana/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: lgtm-grafana
  namespace: default
  labels:
    helm.sh/chart: grafana-8.5.1
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/version: "11.2.0"
    app.kubernetes.io/managed-by: Helm
data:
  
  grafana.ini: |
    [analytics]
    check_for_updates = true
    [dataproxy]
    max_idle_connections = 500
    [grafana_net]
    url = https://grafana.net
    [log]
    mode = console
    [paths]
    data = /var/lib/grafana/
    logs = /var/log/grafana
    plugins = /var/lib/grafana/plugins
    provisioning = /etc/grafana/provisioning
    [server]
    domain = ''
  datasources.yaml: |
    apiVersion: 1
    datasources:
    - access: proxy
      jsonData:
        derivedFields:
        - datasourceUid: tempo
          matcherRegex: traceid
          matcherType: label
          name: TraceID
          url: $${__value.raw}
          urlDisplayLabel: View Trace
        maxLines: 1000
      name: Loki
      type: loki
      uid: loki
      url: http://lgtm-loki:3100
      version: 1
    - access: proxy
      basicAuth: false
      jsonData:
        tracesToLogsV2:
          customQuery: true
          datasourceUid: loki
          filterBySpanID: false
          filterByTraceID: false
          query: '{namespace="$${__span.tags.namespace}", pod="$${__span.tags.pod}"} |="$${__trace.traceId}"'
          spanEndTimeShift: 1s
          spanStartTimeShift: -1s
        tracesToMetrics:
          datasourceUid: mimir
          queries:
          - name: Sample query
            query: sum(rate(process_runtime_jvm_cpu_utilization{$$__tags}[5m]))
          spanEndTimeShift: 1m
          spanStartTimeShift: -1m
          tags:
          - key: app
      name: Tempo
      type: tempo
      uid: tempo
      url: http://lgtm-tempo:3100
    - name: Mimir
      type: prometheus
      uid: mimir
      url: http://lgtm-mimir:9009/prometheus
---
# Source: lgtm-minimal/charts/loki-stack/templates/datasources.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: lgtm-loki-stack
  namespace: default
  labels:
    app: loki-stack
    chart: loki-stack-2.10.2
    release: lgtm
    heritage: Helm
    grafana_datasource: "1"
data:
  loki-stack-datasource.yaml: |-
    apiVersion: 1
    datasources:
    - name: Loki
      type: loki
      access: proxy
      url: "http://lgtm-loki:3100"
      version: 1
      isDefault: true
      jsonData:
        {}
---
# Source: lgtm-minimal/charts/loki-stack/templates/tests/loki-test-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: lgtm-loki-stack-test
  labels:
    app: loki-stack
    chart: loki-stack-2.10.2
    release: lgtm
    heritage: Helm
data:
  test.sh: |
    #!/usr/bin/env bash

    LOKI_URI="http://${LOKI_SERVICE}:${LOKI_PORT}"

    function setup() {
      apk add -u curl jq
      until (curl -s ${LOKI_URI}/loki/api/v1/label/app/values | jq -e '.data[] | select(. == "loki")'); do
        sleep 1
      done
    }

    @test "Has labels" {
      curl -s ${LOKI_URI}/loki/api/v1/labels | \
      jq -e '.data[] | select(. == "app")'
    }

    @test "Query log entry" {
      curl -sG ${LOKI_URI}/api/prom/query?limit=10 --data-urlencode 'query={app="loki"}' | \
      jq -e '.streams[].entries | length >=1'
    }

    @test "Push log entry" {
      local timestamp=$(date +%s000000000)
      local data=$(jq -n --arg timestamp "${timestamp}" '{"streams": [{"stream": {"app": "loki-test"}, "values": [[$timestamp, "foobar"]]}]}')

      curl -s -X POST -H "Content-Type: application/json" ${LOKI_URI}/loki/api/v1/push --data-raw "${data}"

      curl -sG ${LOKI_URI}/loki/api/v1/query_range?limit=1 --data-urlencode 'query={app="loki-test"}' | \
      jq -e '.data.result[].values[][1] == "foobar"'
    }
---
# Source: lgtm-minimal/charts/mimir/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: "mimir-cm"
data:
  mimir.yaml : |
    target: all,alertmanager,overrides-exporter
    multitenancy_enabled: false
    common:
      storage:
        backend: s3
        s3:
          endpoint: "lgtm-minio:9000"
          access_key_id: admin
          secret_access_key: supersecret
          insecure: true
          bucket_name: mimir
    
    # Blocks storage requires a prefix when using a common object storage bucket.
    blocks_storage:
      storage_prefix: blocks
      tsdb:
        dir: /data/ingester
    
    ruler:
      rule_path: /data/ruler
      alertmanager_url: http://127.0.0.1:8080/alertmanager
      ring:
        # Quickly detect unhealthy rulers to speed up the tutorial.
        heartbeat_period: 2s
        heartbeat_timeout: 10s
    
    alertmanager:
      data_dir: /data/alertmanager
      external_url: http://localhost:9009/alertmanager
    
    store_gateway:
      sharding_ring:
        replication_factor: 1
    
    distributor:
      ring:
        instance_addr: 127.0.0.1
        kvstore:
          store: memberlist
    
    ingester:
      ring:
        instance_addr: 127.0.0.1
        kvstore:
          store: memberlist
        replication_factor: 1
    
    limits:
      native_histograms_ingestion_enabled: true
    
    server:
      http_listen_port: 9009
      log_level: error
---
# Source: lgtm-minimal/charts/tempo/templates/configmap-tempo.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: tempo
  namespace: default
  labels:
    helm.sh/chart: tempo-1.10.3
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/version: "2.5.0"
    app.kubernetes.io/managed-by: Helm
data:
  overrides.yaml: |
    overrides:
      {}
  tempo.yaml: |
    multitenancy_enabled: false
    usage_report:
      reporting_enabled: true
    compactor:
      compaction:
        block_retention: 24h
    distributor:
      receivers:
            jaeger:
              protocols:
                grpc:
                  endpoint: 0.0.0.0:14250
                thrift_binary:
                  endpoint: 0.0.0.0:6832
                thrift_compact:
                  endpoint: 0.0.0.0:6831
                thrift_http:
                  endpoint: 0.0.0.0:14268
            otlp:
              protocols:
                grpc:
                  endpoint: 0.0.0.0:4317
                http:
                  endpoint: 0.0.0.0:4318
    ingester:
          {}
    server:
          http_listen_port: 3100
    storage:
          trace:
            backend: local
            local:
              path: /var/tempo/traces
            wal:
              path: /var/tempo/wal
    querier:
          {}
    query_frontend:
          {}
    overrides:
          per_tenant_override_config: /conf/overrides.yaml
---
# Source: lgtm-minimal/charts/minio/templates/pvc.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: lgtm-minio
  namespace: "default"
  labels:
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: minio
    app.kubernetes.io/version: 2024.9.22
    helm.sh/chart: minio-14.7.11
spec:
  accessModes:
    - "ReadWriteOnce"
  resources:
    requests:
      storage: "8Gi"
---
# Source: lgtm-minimal/charts/grafana/templates/clusterrole.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  labels:
    helm.sh/chart: grafana-8.5.1
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/version: "11.2.0"
    app.kubernetes.io/managed-by: Helm
  name: lgtm-grafana-clusterrole
rules:
  - apiGroups: [""] # "" indicates the core API group
    resources: ["configmaps", "secrets"]
    verbs: ["get", "watch", "list"]
---
# Source: lgtm-minimal/charts/grafana/templates/clusterrolebinding.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: lgtm-grafana-clusterrolebinding
  labels:
    helm.sh/chart: grafana-8.5.1
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/version: "11.2.0"
    app.kubernetes.io/managed-by: Helm
subjects:
  - kind: ServiceAccount
    name: lgtm-grafana
    namespace: default
roleRef:
  kind: ClusterRole
  name: lgtm-grafana-clusterrole
  apiGroup: rbac.authorization.k8s.io
---
# Source: lgtm-minimal/charts/grafana/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: lgtm-grafana
  namespace: default
  labels:
    helm.sh/chart: grafana-8.5.1
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/version: "11.2.0"
    app.kubernetes.io/managed-by: Helm
rules: []
---
# Source: lgtm-minimal/charts/loki-stack/charts/loki/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: lgtm-loki
  namespace: default
  labels:
    app: loki
    chart: loki-2.16.0
    release: lgtm
    heritage: Helm
rules:
- apiGroups:      ['extensions']
  resources:      ['podsecuritypolicies']
  verbs:          ['use']
  resourceNames:  [lgtm-loki]
---
# Source: lgtm-minimal/charts/grafana/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: lgtm-grafana
  namespace: default
  labels:
    helm.sh/chart: grafana-8.5.1
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/version: "11.2.0"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: lgtm-grafana
subjects:
- kind: ServiceAccount
  name: lgtm-grafana
  namespace: default
---
# Source: lgtm-minimal/charts/loki-stack/charts/loki/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: lgtm-loki
  namespace: default
  labels:
    app: loki
    chart: loki-2.16.0
    release: lgtm
    heritage: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: lgtm-loki
subjects:
- kind: ServiceAccount
  name: lgtm-loki
---
# Source: lgtm-minimal/charts/grafana/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: lgtm-grafana
  namespace: default
  labels:
    helm.sh/chart: grafana-8.5.1
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/version: "11.2.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: service
      port: 80
      protocol: TCP
      targetPort: 3000
  selector:
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: lgtm
---
# Source: lgtm-minimal/charts/loki-stack/charts/loki/templates/service-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: lgtm-loki-headless
  namespace: default
  labels:
    app: loki
    chart: loki-2.16.0
    release: lgtm
    heritage: Helm
    variant: headless
spec:
  clusterIP: None
  ports:
    - port: 3100
      protocol: TCP
      name: http-metrics
      targetPort: http-metrics
  selector:
    app: loki
    release: lgtm
---
# Source: lgtm-minimal/charts/loki-stack/charts/loki/templates/service-memberlist.yaml
apiVersion: v1
kind: Service
metadata:
  name: lgtm-loki-memberlist
  namespace: default
  labels:
    app: loki
    chart: loki-2.16.0
    release: lgtm
    heritage: Helm
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - name: http
      port: 7946
      targetPort: memberlist-port
      protocol: TCP
  selector:
    app: loki
    release: lgtm
---
# Source: lgtm-minimal/charts/loki-stack/charts/loki/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: lgtm-loki
  namespace: default
  labels:
    app: loki
    chart: loki-2.16.0
    release: lgtm
    heritage: Helm
  annotations:
    {}
spec:
  type: ClusterIP
  ports:
    - port: 3100
      protocol: TCP
      name: http-metrics
      targetPort: http-metrics
  selector:
    app: loki
    release: lgtm
---
# Source: lgtm-minimal/charts/mimir/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: lgtm-mimir
  labels:
    helm.sh/chart: mimir-0.1.0
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/version: "2.13.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 9009
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: lgtm
---
# Source: lgtm-minimal/charts/minio/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: lgtm-minio
  namespace: "default"
  labels:
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: minio
    app.kubernetes.io/version: 2024.9.22
    helm.sh/chart: minio-14.7.11
spec:
  type: ClusterIP
  ports:
    - name: minio-api
      port: 9000
      targetPort: minio-api
      nodePort: null
    - name: minio-console
      port: 9001
      targetPort: minio-console
      nodePort: null
  selector:
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/name: minio
---
# Source: lgtm-minimal/charts/tempo/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: lgtm-tempo
  namespace: default
  labels:
    helm.sh/chart: tempo-1.10.3
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/version: "2.5.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
  - name: tempo-prom-metrics
    port: 3100
    targetPort: 3100
  - name: tempo-jaeger-thrift-compact
    port: 6831
    protocol: UDP
    targetPort: 6831
  - name: tempo-jaeger-thrift-binary
    port: 6832
    protocol: UDP
    targetPort: 6832
  - name: tempo-jaeger-thrift-http
    port: 14268
    protocol: TCP
    targetPort: 14268
  - name: grpc-tempo-jaeger
    port: 14250
    protocol: TCP
    targetPort: 14250
  - name: tempo-zipkin
    port: 9411
    protocol: TCP
    targetPort: 9411
  - name: tempo-otlp-legacy
    port: 55680
    protocol: TCP
    targetPort: 55680
  - name: tempo-otlp-http-legacy
    port: 55681
    protocol: TCP
    targetPort: 4318
  - name: grpc-tempo-otlp
    port: 4317
    protocol: TCP
    targetPort: 4317
  - name: tempo-otlp-http
    port: 4318
    protocol: TCP
    targetPort: 4318
  - name: tempo-opencensus
    port: 55678
    protocol: TCP
    targetPort: 55678
  selector:
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: lgtm
---
# Source: lgtm-minimal/charts/grafana/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: lgtm-grafana
  namespace: default
  labels:
    helm.sh/chart: grafana-8.5.1
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/version: "11.2.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: grafana
      app.kubernetes.io/instance: lgtm
  strategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: grafana
        app.kubernetes.io/instance: lgtm
      annotations:
        checksum/config: 9e981a9980a5d04c727f944383c3161e3cfdfd61d7d188e012c18e63eb3c737d
        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24
        checksum/secret: bed677784356b2af7fb0d87455db21f077853059b594101a4f6532bfbd962a7f
        kubectl.kubernetes.io/default-container: grafana
    spec:
      
      serviceAccountName: lgtm-grafana
      automountServiceAccountToken: true
      securityContext:
        fsGroup: 472
        runAsGroup: 472
        runAsNonRoot: true
        runAsUser: 472
      enableServiceLinks: true
      containers:
        - name: grafana-sc-dashboard
          image: "quay.io/kiwigrid/k8s-sidecar:1.27.4"
          imagePullPolicy: IfNotPresent
          env:
            - name: METHOD
              value: WATCH
            - name: LABEL
              value: "grafana_dashboard"
            - name: FOLDER
              value: "/tmp/dashboards"
            - name: RESOURCE
              value: "both"
            - name: REQ_USERNAME
              valueFrom:
                secretKeyRef:
                  name: lgtm-grafana
                  key: admin-user
            - name: REQ_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: lgtm-grafana
                  key: admin-password
            - name: REQ_URL
              value: http://localhost:3000/api/admin/provisioning/dashboards/reload
            - name: REQ_METHOD
              value: POST
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - name: sc-dashboard-volume
              mountPath: "/tmp/dashboards"
        - name: grafana-sc-datasources
          image: "quay.io/kiwigrid/k8s-sidecar:1.27.4"
          imagePullPolicy: IfNotPresent
          env:
            - name: METHOD
              value: WATCH
            - name: LABEL
              value: "grafana_datasource"
            - name: FOLDER
              value: "/etc/grafana/provisioning/datasources"
            - name: RESOURCE
              value: "both"
            - name: REQ_USERNAME
              valueFrom:
                secretKeyRef:
                  name: lgtm-grafana
                  key: admin-user
            - name: REQ_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: lgtm-grafana
                  key: admin-password
            - name: REQ_URL
              value: http://localhost:3000/api/admin/provisioning/datasources/reload
            - name: REQ_METHOD
              value: POST
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - name: sc-datasources-volume
              mountPath: "/etc/grafana/provisioning/datasources"
        - name: grafana
          image: "docker.io/grafana/grafana:11.2.0"
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - name: config
              mountPath: "/etc/grafana/grafana.ini"
              subPath: grafana.ini
            - name: storage
              mountPath: "/var/lib/grafana"
            - name: config
              mountPath: "/etc/grafana/provisioning/datasources/datasources.yaml"
              subPath: "datasources.yaml"
            - name: sc-dashboard-volume
              mountPath: "/tmp/dashboards"
            - name: sc-dashboard-provider
              mountPath: "/etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml"
              subPath: provider.yaml
            - name: sc-datasources-volume
              mountPath: "/etc/grafana/provisioning/datasources"
          ports:
            - name: grafana
              containerPort: 3000
              protocol: TCP
            - name: gossip-tcp
              containerPort: 9094
              protocol: TCP
            - name: gossip-udp
              containerPort: 9094
              protocol: UDP
          env:
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: GF_SECURITY_ADMIN_USER
              valueFrom:
                secretKeyRef:
                  name: lgtm-grafana
                  key: admin-user
            - name: GF_SECURITY_ADMIN_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: lgtm-grafana
                  key: admin-password
            - name: GF_PATHS_DATA
              value: /var/lib/grafana/
            - name: GF_PATHS_LOGS
              value: /var/log/grafana
            - name: GF_PATHS_PLUGINS
              value: /var/lib/grafana/plugins
            - name: GF_PATHS_PROVISIONING
              value: /etc/grafana/provisioning
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
            initialDelaySeconds: 60
            timeoutSeconds: 30
          readinessProbe:
            httpGet:
              path: /api/health
              port: 3000
      volumes:
        - name: config
          configMap:
            name: lgtm-grafana
        - name: storage
          emptyDir: {}
        - name: sc-dashboard-volume
          emptyDir:
            {}
        - name: sc-dashboard-provider
          configMap:
            name: lgtm-grafana-config-dashboards
        - name: sc-datasources-volume
          emptyDir:
            {}
---
# Source: lgtm-minimal/charts/mimir/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: lgtm-mimir
  labels:
    helm.sh/chart: mimir-0.1.0
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/version: "2.13.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: mimir
      app.kubernetes.io/instance: lgtm
  template:
    metadata:
      labels:
        helm.sh/chart: mimir-0.1.0
        app.kubernetes.io/name: mimir
        app.kubernetes.io/instance: lgtm
        app.kubernetes.io/version: "2.13.0"
        app.kubernetes.io/managed-by: Helm
    spec:
      serviceAccountName: default
      securityContext:
        {}
      containers:
        - name: mimir
          securityContext:
            {}
          image: "grafana/mimir:2.13.0"
          imagePullPolicy: IfNotPresent
          args:
            - "-target=all"
            - "-config.expand-env=true"
            - "-config.file=/etc/mimir/mimir.yaml"
          ports:
            - name: http
              containerPort: 9009
              protocol: TCP
          resources:
            {}
          volumeMounts:
            - mountPath: /etc/mimir
              name: config
      volumes:
        - configMap:
            name: mimir-cm
          name: config
---
# Source: lgtm-minimal/charts/minio/templates/standalone/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: lgtm-minio
  namespace: "default"
  labels:
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: minio
    app.kubernetes.io/version: 2024.9.22
    helm.sh/chart: minio-14.7.11
spec:
  selector:
    matchLabels:
      app.kubernetes.io/instance: lgtm
      app.kubernetes.io/name: minio
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: lgtm
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: minio
        app.kubernetes.io/version: 2024.9.22
        helm.sh/chart: minio-14.7.11
      annotations:
        checksum/credentials-secret: 6e16371a1aa2963f5544526b346585f3ebc6f89b4696ab973d82a166adf96ff4
    spec:
      
      serviceAccountName: lgtm-minio
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: lgtm
                    app.kubernetes.io/name: minio
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      automountServiceAccountToken: false
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: OnRootMismatch
        supplementalGroups: []
        sysctls: []
      containers:
        - name: minio
          image: docker.io/bitnami/minio:2024.3.15-debian-12-r0
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MINIO_SCHEME
              value: "http"
            - name: MINIO_FORCE_NEW_KEYS
              value: "no"
            - name: MINIO_API_PORT_NUMBER
              value: "9000"
            - name: MINIO_ROOT_USER
              valueFrom:
                secretKeyRef:
                  name: lgtm-minio
                  key: root-user
            - name: MINIO_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: lgtm-minio
                  key: root-password
            - name: MINIO_DEFAULT_BUCKETS
              value: loki, mimir, tempo
            - name: MINIO_BROWSER
              value: "on"
            - name: MINIO_PROMETHEUS_AUTH_TYPE
              value: "public"
            - name: MINIO_CONSOLE_PORT_NUMBER
              value: "9001"
            - name: MINIO_DATA_DIR
              value: "/bitnami/minio/data"
          envFrom:
          ports:
            - name: minio-api
              containerPort: 9000
              protocol: TCP
            - name: minio-console
              containerPort: 9001
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /minio/health/live
              port: minio-api
              scheme: "HTTP"
            initialDelaySeconds: 5
            periodSeconds: 5
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 5
          readinessProbe:
            tcpSocket:
              port: minio-api
            initialDelaySeconds: 5
            periodSeconds: 5
            timeoutSeconds: 1
            successThreshold: 1
            failureThreshold: 5
          resources:
            limits:
              cpu: 375m
              ephemeral-storage: 2Gi
              memory: 384Mi
            requests:
              cpu: 250m
              ephemeral-storage: 50Mi
              memory: 256Mi
          volumeMounts:
            - name: empty-dir
              mountPath: /tmp
              subPath: tmp-dir
            - name: empty-dir
              mountPath: /opt/bitnami/minio/tmp
              subPath: app-tmp-dir
            - name: empty-dir
              mountPath: /.mc
              subPath: app-mc-dir
            - name: data
              mountPath: /bitnami/minio/data
      volumes:
        - name: empty-dir
          emptyDir: {}
        - name: data
          persistentVolumeClaim:
            claimName: lgtm-minio
---
# Source: lgtm-minimal/charts/loki-stack/charts/loki/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: lgtm-loki
  namespace: default
  labels:
    app: loki
    chart: loki-2.16.0
    release: lgtm
    heritage: Helm
  annotations:
    {}
spec:
  podManagementPolicy: OrderedReady
  replicas: 1
  selector:
    matchLabels:
      app: loki
      release: lgtm
  serviceName: lgtm-loki-headless
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: loki
        name: lgtm-loki
        release: lgtm
      annotations:
        checksum/config: 59f24f3b239f2420037c28836a29a71045980431e5d8a9591e5f0041fe877d7e
        prometheus.io/port: http-metrics
        prometheus.io/scrape: "true"
    spec:
      serviceAccountName: lgtm-loki
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
      initContainers:
        []
      containers:
        - name: loki
          image: "grafana/loki:2.8.1"
          imagePullPolicy: IfNotPresent
          args:
            - "-config.file=/etc/loki/loki.yaml"
          volumeMounts:
            - name: tmp
              mountPath: /tmp
            - name: config
              mountPath: /etc/loki
            - name: storage
              mountPath: "/data"
              subPath: 
          ports:
            - name: http-metrics
              containerPort: 3100
              protocol: TCP
            - name: grpc
              containerPort: 9095
              protocol: TCP
            - name: memberlist-port
              containerPort: 7946
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /ready
              port: http-metrics
            initialDelaySeconds: 45
          readinessProbe:
            httpGet:
              path: /ready
              port: http-metrics
            initialDelaySeconds: 45
          resources:
            {}
          securityContext:
            readOnlyRootFilesystem: true
          env:
      nodeSelector:
        {}
      affinity:
        {}
      tolerations:
        []
      terminationGracePeriodSeconds: 4800
      volumes:
        - name: tmp
          emptyDir: {}
        - name: config
          secret:
            secretName: lgtm-loki
        - name: storage
          emptyDir: {}
---
# Source: lgtm-minimal/charts/tempo/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: lgtm-tempo
  namespace: default
  labels:
    helm.sh/chart: tempo-1.10.3
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/version: "2.5.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: tempo
      app.kubernetes.io/instance: lgtm
  serviceName: lgtm-tempo-headless
  template:
    metadata:
      labels:
        app.kubernetes.io/name: tempo
        app.kubernetes.io/instance: lgtm
      annotations:
        checksum/config: 884e7e8946e63c1739ed4b3bc54a4454d49a598164bc18c2f32b559230c6317a
    spec:
      serviceAccountName: lgtm-tempo
      automountServiceAccountToken: true
      containers:
      - args:
        - -config.file=/conf/tempo.yaml
        - -mem-ballast-size-mbs=1024
        image: grafana/tempo:2.5.0
        imagePullPolicy: IfNotPresent
        name: tempo
        ports:
        - containerPort: 3100
          name: prom-metrics
        - containerPort: 6831
          name: jaeger-thrift-c
          protocol: UDP
        - containerPort: 6832
          name: jaeger-thrift-b
          protocol: UDP
        - containerPort: 14268
          name: jaeger-thrift-h
        - containerPort: 14250
          name: jaeger-grpc
        - containerPort: 9411
          name: zipkin
        - containerPort: 55680
          name: otlp-legacy
        - containerPort: 4317
          name: otlp-grpc
        - containerPort: 55681
          name: otlp-httplegacy
        - containerPort: 4318
          name: otlp-http
        - containerPort: 55678
          name: opencensus
        resources:
          {}
        env:
        volumeMounts:
        - mountPath: /conf
          name: tempo-conf
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
      volumes:
      - configMap:
          name: tempo
        name: tempo-conf
  updateStrategy:
    type:
      RollingUpdate
---
# Source: lgtm-minimal/charts/grafana/templates/tests/test-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    helm.sh/chart: grafana-8.5.1
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/version: "11.2.0"
    app.kubernetes.io/managed-by: Helm
  name: lgtm-grafana-test
  namespace: default
  annotations:
    "helm.sh/hook": test
    "helm.sh/hook-delete-policy": "before-hook-creation,hook-succeeded"
---
# Source: lgtm-minimal/charts/grafana/templates/tests/test-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: lgtm-grafana-test
  namespace: default
  annotations:
    "helm.sh/hook": test
    "helm.sh/hook-delete-policy": "before-hook-creation,hook-succeeded"
  labels:
    helm.sh/chart: grafana-8.5.1
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/version: "11.2.0"
    app.kubernetes.io/managed-by: Helm
data:
  run.sh: |-
    @test "Test Health" {
      url="http://lgtm-grafana/api/health"

      code=$(wget --server-response --spider --timeout 90 --tries 10 ${url} 2>&1 | awk '/^  HTTP/{print $2}')
      [ "$code" == "200" ]
    }
---
# Source: lgtm-minimal/charts/grafana/templates/tests/test.yaml
apiVersion: v1
kind: Pod
metadata:
  name: lgtm-grafana-test
  labels:
    helm.sh/chart: grafana-8.5.1
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/version: "11.2.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
    "helm.sh/hook-delete-policy": "before-hook-creation,hook-succeeded"
  namespace: default
spec:
  serviceAccountName: lgtm-grafana-test
  containers:
    - name: lgtm-test
      image: "docker.io/bats/bats:v1.4.1"
      imagePullPolicy: "IfNotPresent"
      command: ["/opt/bats/bin/bats", "-t", "/tests/run.sh"]
      volumeMounts:
        - mountPath: /tests
          name: tests
          readOnly: true
  volumes:
    - name: tests
      configMap:
        name: lgtm-grafana-test
  restartPolicy: Never
---
# Source: lgtm-minimal/charts/loki-stack/templates/tests/loki-test-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    "helm.sh/hook": test-success
  labels:
    app: loki-stack
    chart: loki-stack-2.10.2
    release: lgtm
    heritage: Helm
  name: lgtm-loki-stack-test
spec:
  containers:
  - name: test
    image: "bats/bats:1.8.2"
    imagePullPolicy: ""
    args:
    - /var/lib/loki/test.sh
    env:
    - name: LOKI_SERVICE
      value: lgtm-loki
    - name: LOKI_PORT
      value: "3100"
    volumeMounts:
    - name: tests
      mountPath: /var/lib/loki
  restartPolicy: Never
  volumes:
  - name: tests
    configMap:
      name: lgtm-loki-stack-test
